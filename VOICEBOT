import speech_recognition as sr
import pyttsx3
import openai
import os
import librosa
import numpy as np
import tempfile
import soundfile as sf
from transformers import Wav2Vec2ForSequenceClassification, Wav2Vec2Processor
import torch

openai.api_key = os.environ.get("Your_open_AI_API_key")

engine = pyttsx3.init()

# Initialize Hugging Face model and processor
model_name = "ehcalabres/wav2vec2-lg-xlsr-en-speech-emotion-recognition"
processor = Wav2Vec2Processor.from_pretrained(model_name)
model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)

def customize_tts():
    voices = engine.getProperty('voices')
    engine.setProperty('voice', voices[1].id)  # Choose voice (e.g., index 1 for female)
    engine.setProperty('rate', 150)  # Adjust rate (speed of speech)
    engine.setProperty('volume', 1.0)  # Set volume to maximum

def speak(text):
    engine.say(text)
    engine.runAndWait()

def extract_emotion_from_audio(audio_data, sr):
    # Convert audio data to the format the model expects
    input_values = processor(audio_data, return_tensors="pt", sampling_rate=sr).input_values

    # Predict emotion
    with torch.no_grad():
        logits = model(input_values).logits

    # Map logits to emotions
    predicted_class = torch.argmax(logits, dim=-1).item()
    emotions = ["neutral", "happy", "sad", "angry", "fearful", "disgust", "surprised"]  # Add relevant emotions
    return emotions[predicted_class]

def respond_emotionally(text, emotion):
    if emotion == "happy":
        return "You sound cheerful! " + text
    elif emotion == "sad":
        return "I’m here for you. " + text
    elif emotion == "angry":
        return "Let’s try to sort this out calmly. " + text
    elif emotion == "fearful":
        return "I understand you're anxious. Let's take it easy. " + text
    elif emotion == "disgust":
        return "I can sense you're not happy. Let's talk it through. " + text
    elif emotion == "surprised":
        return "You seem surprised! " + text
    else:
        return text

def listen_and_detect_emotion(recognizer, mic):
    with mic as source:
        print("Listening...")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=10)
            audio_data = audio.get_wav_data()
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_wav:
                tmp_wav.write(audio_data)
                tmp_wav_path = tmp_wav.name

            y, sr_lib = librosa.load(tmp_wav_path, sr=None)
            emotion = extract_emotion_from_audio(y, sr_lib)

            print(f"Detected Emotion: {emotion}")
            print("Recognizing...")
            text = recognizer.recognize_google(audio)
            print(f"You said: {text}")
            return text, emotion
        except sr.UnknownValueError:
            speak("Sorry, I could not understand the audio.")
        except sr.RequestError:
            speak("There was an error with the speech recognition service.")
        except sr.WaitTimeoutError:
            speak("You didn't say anything.")
        return None, "neutral"

def listen_with_retry(recognizer, mic, max_attempts=3):
    attempts = 0
    while attempts < max_attempts:
        text, emotion = listen_and_detect_emotion(recognizer, mic)
        if text:
            return text, emotion
        else:
            speak("I didn't catch that. Could you try again?")
        attempts += 1
    return None, "neutral"

def generate_response(prompt, message_history):
    try:
        message_history.append({"role": "user", "content": prompt})

        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=message_history,
            request_timeout=10
        )

        reply = response['choices'][0]['message']['content']
        message_history.append({"role": "assistant", "content": reply})

        # Manage history size
        MAX_HISTORY = 10
        if len(message_history) > 2 * MAX_HISTORY + 1:
            message_history = [message_history[0]] + message_history[-2 * MAX_HISTORY:]
            print("Notice: Old messages were removed.")
            speak("I’ve cleaned up our chat history to stay efficient.")

        return reply
    except Exception as e:
        print(f"Error generating response: {e}")
        speak("There was an error generating a response.")
        return None

def main():
    message_history = [{"role": "system", "content": "You are a helpful and conversational voice assistant."}]
    customize_tts()

    recognizer = sr.Recognizer()
    mic = sr.Microphone()

    with mic as source:
        print("Calibrating for ambient noise...")
        recognizer.adjust_for_ambient_noise(source, duration=1)
        print("Calibration complete.")
    speak("Hello! I'm your assistant. How can I help you today?")

    while True:
        user_input, emotion = listen_with_retry(recognizer, mic)
        if user_input:
            if user_input.lower() in ["exit", "quit", "stop", "bye", "goodbye"]:
                speak("Goodbye!")
                break
            response = generate_response(user_input, message_history)
            if response:
                emotional_reply = respond_emotionally(response, emotion)
                print(f"Assistant: {emotional_reply}")
                speak(emotional_reply)

if __name__ == "__main__":
    main()
