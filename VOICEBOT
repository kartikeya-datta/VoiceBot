import speech_recognition as sr
import pyttsx3
import openai
import os
import librosa
import numpy as np
import tempfile
import soundfile as sf

openai.api_key = os.environ.get("Your_open_AI_API_key")

engine = pyttsx3.init()

def customize_tts():
    voices = engine.getProperty('voices')
    engine.setProperty('voice', voices[1].id) 
    engine.setProperty('rate', 150)
    engine.setProperty('volume', 1.0)

def speak(text):
    engine.say(text)
    engine.runAndWait()

def extract_emotion_features(audio_data, sr):
    mfccs = librosa.feature.mfcc(y=audio_data, sr=sr, n_mfcc=13)
    mfccs_mean = np.mean(mfccs, axis=1)
    zcr = np.mean(librosa.feature.zero_crossing_rate(audio_data))
    energy = np.mean(librosa.feature.rms(y=audio_data))
    return np.concatenate((mfccs_mean, [zcr, energy]))

def classify_emotion(audio_data, sr):
    features = extract_emotion_features(audio_data, sr)
    energy = features[-1]

    if energy > 0.06:
        return "happy"
    elif energy < 0.02:
        return "sad"
    elif features[-2] > 0.12:
        return "angry"
    else:
        return "neutral"

def respond_emotionally(text, emotion):
    if emotion == "happy":
        return "You sound cheerful! " + text
    elif emotion == "sad":
        return "I’m here for you. " + text
    elif emotion == "angry":
        return "Let’s try to sort this out calmly. " + text
    else:
        return text

def listen_and_detect_emotion(recognizer, mic):
    with mic as source:
        print("Listening...")
        try:
            audio = recognizer.listen(source, timeout=5, phrase_time_limit=10)
            audio_data = audio.get_wav_data()
            with tempfile.NamedTemporaryFile(delete=False, suffix=".wav") as tmp_wav:
                tmp_wav.write(audio_data)
                tmp_wav_path = tmp_wav.name

            y, sr_lib = librosa.load(tmp_wav_path, sr=None)
            emotion = classify_emotion(y, sr_lib)

            print(f"Detected Emotion: {emotion}")
            print("Recognizing...")
            text = recognizer.recognize_google(audio)
            print(f"You said: {text}")
            return text, emotion
        except sr.UnknownValueError:
            speak("Sorry, I could not understand the audio.")
        except sr.RequestError:
            speak("There was an error with the speech recognition service.")
        except sr.WaitTimeoutError:
            speak("You didn't say anything.")
        return None, "neutral"

def listen_with_retry(recognizer, mic, max_attempts=3):
    attempts = 0
    while attempts < max_attempts:
        text, emotion = listen_and_detect_emotion(recognizer, mic)
        if text:
            return text, emotion
        else:
            speak("I didn't catch that. Could you try again?")
        attempts += 1
    return None, "neutral"

def generate_response(prompt, message_history):
    try:
        message_history.append({"role": "user", "content": prompt})

        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",
            messages=message_history,
            request_timeout=10
        )

        reply = response['choices'][0]['message']['content']
        message_history.append({"role": "assistant", "content": reply})

        # Manage history size
        MAX_HISTORY = 10
        if len(message_history) > 2 * MAX_HISTORY + 1:
            message_history = [message_history[0]] + message_history[-2 * MAX_HISTORY:]
            print("Notice: Old messages were removed.")
            speak("I’ve cleaned up our chat history to stay efficient.")

        return reply
    except Exception as e:
        print(f"Error generating response: {e}")
        speak("There was an error generating a response.")
        return None

def main():
    message_history = [{"role": "system", "content": "You are a helpful and conversational voice assistant."}]
    customize_tts()

    recognizer = sr.Recognizer()
    mic = sr.Microphone()

    with mic as source:
        print("Calibrating for ambient noise...")
        recognizer.adjust_for_ambient_noise(source, duration=1)
        print("Calibration complete.")
    speak("Hello! I'm your assistant. How can I help you today?")

    while True:
        user_input, emotion = listen_with_retry(recognizer, mic)
        if user_input:
            if user_input.lower() in ["exit", "quit", "stop", "bye", "goodbye"]:
                speak("Goodbye!")
                break
            response = generate_response(user_input, message_history)
            if response:
                emotional_reply = respond_emotionally(response, emotion)
                print(f"Assistant: {emotional_reply}")
                speak(emotional_reply)

if __name__ == "__main__":
    main()
